{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67d98YQ9Em9J"
   },
   "source": [
    "# üìú Projeto Final - Capacita√ß√£o IA (Ciclo 3)\n",
    "# üéì Alunos: Filipe da Silva Rodrigues e Rodrigo Serafim Floriano da Silva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T3nUxV-Em9L"
   },
   "source": "## üìö Bibliotecas Necess√°rias"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:51:04.310177Z",
     "start_time": "2024-11-20T16:51:04.307206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instala√ß√£o de bibliotecas necess√°rias para execu√ß√£o do c√≥digo\n",
    "# !pip install numpy pandas scikit-learn mlflow xgboost lightgbm catboost tpot pytorch auto-sklearn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xx3sAPU_Em9L",
    "ExecuteTime": {
     "end_time": "2024-11-21T00:53:10.964972Z",
     "start_time": "2024-11-21T00:53:10.958974Z"
    }
   },
   "source": [
    "# Tratamento de Dataset e M√©tricas\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "# Modelos de Treinamento\n",
    "\n",
    "# Classificadores b√°sicos\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Classificadores avan√ßados\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# AutoML\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Armazenamento e An√°lise de Modelos\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.pyfunc import PythonModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Terminal\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rieolC1PEm9M"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "üëæ **Dataset de Classifica√ß√£o - Kaggle: Water Quality**\n",
    "\n",
    "Esse dataframe √© um conjunto de dados que cont√©m informa√ß√µes sobre a qualidade da √°gua e sua potabilidade. As vari√°veis s√£o:\n",
    "\n",
    "- `ph`: o valor do pH da √°gua (0 a 14).\n",
    "- `Hardness`: a capacidade da √°gua de precipitar sab√£o em mg/L.\n",
    "- `Solids`: s√≥lidos totais dissolvidos em ppm.\n",
    "- `Chloramines`: quantidade de cloraminas em ppm.\n",
    "- `Sulfate`: quantidade de sulfatos dissolvidos em mg/L.\n",
    "- `Conductivity`: condutividade el√©trica da √°gua em ŒºS/cm.\n",
    "- `Organic_carbon`: quantidade de carbono org√¢nico em ppm.\n",
    "- `Trihalomethanes`: quantidade de trihalometanos em Œºg/L.\n",
    "- `Turbidity`: medida da propriedade de emiss√£o de luz da √°gua em NTU.\n",
    "- `Potability`: indica se a √°gua √© segura para consumo humano (1 = Pot√°vel, 0 = N√£o pot√°vel).\n",
    "\n",
    "‚úÖ **Objetivo:** Prever se a √°gua √© pot√°vel ou n√£o com base nas caracter√≠sticas coletadas.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:51:09.349689Z",
     "start_time": "2024-11-20T16:51:09.293565Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_F2jeyjEm9M",
    "outputId": "50e652ed-7ad9-49ca-eac7-c8c045500bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Informa√ß√µes do Dataset:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               2785 non-null   float64\n",
      " 1   Hardness         3276 non-null   float64\n",
      " 2   Solids           3276 non-null   float64\n",
      " 3   Chloramines      3276 non-null   float64\n",
      " 4   Sulfate          2495 non-null   float64\n",
      " 5   Conductivity     3276 non-null   float64\n",
      " 6   Organic_carbon   3276 non-null   float64\n",
      " 7   Trihalomethanes  3114 non-null   float64\n",
      " 8   Turbidity        3276 non-null   float64\n",
      " 9   Potability       3276 non-null   int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 256.1 KB\n",
      "None\n",
      "\n",
      "Verificar Valores Nulos:\n",
      "\n",
      "ph                 491\n",
      "Hardness             0\n",
      "Solids               0\n",
      "Chloramines          0\n",
      "Sulfate            781\n",
      "Conductivity         0\n",
      "Organic_carbon       0\n",
      "Trihalomethanes    162\n",
      "Turbidity            0\n",
      "Potability           0\n",
      "dtype: int64\n",
      "\n",
      "Dataset Original:\n",
      "\n",
      "         ph    Hardness        Solids  Chloramines     Sulfate  Conductivity  \\\n",
      "0       NaN  204.890455  20791.318981     7.300212  368.516441    564.308654   \n",
      "1  3.716080  129.422921  18630.057858     6.635246         NaN    592.885359   \n",
      "2  8.099124  224.236259  19909.541732     9.275884         NaN    418.606213   \n",
      "3  8.316766  214.373394  22018.417441     8.059332  356.886136    363.266516   \n",
      "4  9.092223  181.101509  17978.986339     6.546600  310.135738    398.410813   \n",
      "\n",
      "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
      "0       10.379783        86.990970   2.963135           0  \n",
      "1       15.180013        56.329076   4.500656           0  \n",
      "2       16.868637        66.420093   3.055934           0  \n",
      "3       18.436524       100.341674   4.628771           0  \n",
      "4       11.558279        31.997993   4.075075           0  \n",
      "\n",
      "Distribui√ß√£o de Classes do Dataset:\n",
      "\n",
      "Potability\n",
      "0    1998\n",
      "1    1278\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Tamanho do conjunto de treinamento: (3196, 9)\n",
      "Tamanho do conjunto de teste: (656, 9)\n"
     ]
    }
   ],
   "source": [
    "# Carregar o dataset\n",
    "url = 'water_potability.csv'\n",
    "dataset = pd.read_csv(url)\n",
    "\n",
    "# Analisar o dataset\n",
    "print('\\nInforma√ß√µes do Dataset:\\n')\n",
    "print(dataset.info())\n",
    "\n",
    "print('\\nVerificar Valores Nulos:\\n')\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "# Exibir o dataset original\n",
    "print('\\nDataset Original:\\n')\n",
    "print(dataset.head())\n",
    "\n",
    "# Criar uma c√≥pia do dataset para efetuar os devidos tratamentos\n",
    "df = dataset.copy()\n",
    "\n",
    "# Lidar com dados quem possuem valores nulos\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "columns_to_normalize = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']\n",
    "df[columns_to_normalize] = imputer.fit_transform(df[columns_to_normalize])\n",
    "\n",
    "# Normalizar os dados (0 a 1)\n",
    "scaler = MinMaxScaler()  # Alternativa: StandardScaler()\n",
    "df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "\n",
    "# Separar os dados para an√°lise de features e target\n",
    "target = df['Potability'].copy()\n",
    "features = df.drop('Potability', axis=1).copy()\n",
    "\n",
    "print(\"\\nDistribui√ß√£o de Classes do Dataset:\\n\")\n",
    "print(target.value_counts())\n",
    "\n",
    "# Separar dados para treino e teste\n",
    "X_train, x_test, Y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=None, stratify=target)\n",
    "\n",
    "# Balanceamento de classes com SMOTE no conjunto de treinamento\n",
    "smote = SMOTE(random_state=42)\n",
    "x_train, y_train = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "\n",
    "print(f\"\\nTamanho do conjunto de treinamento: {x_train.shape}\")\n",
    "print(f\"Tamanho do conjunto de teste: {x_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctiG4OLS83a7"
   },
   "source": [
    "## üß™ Experimentos no MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:51:09.419180Z",
     "start_time": "2024-11-20T16:51:09.406180Z"
    },
    "id": "pA1lhima83bB"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = {\n",
    "    \"TPOT AutoML\": [\n",
    "        # Configura√ß√£o com foco em explora√ß√£o profunda e diversidade elevada\n",
    "        {\n",
    "            \"generations\": 25,            # Aumenta o n√∫mero de gera√ß√µes para uma explora√ß√£o mais profunda do espa√ßo de hiperpar√¢metros\n",
    "            \"population_size\": 200,       # Maior diversidade de indiv√≠duos na popula√ß√£o, aumentando a capacidade de encontrar boas solu√ß√µes\n",
    "            \"verbosity\": 2,               # Exibe informa√ß√µes detalhadas durante o processo de evolu√ß√£o\n",
    "            \"scoring\": \"f1_weighted\",     # F1 Score como m√©trica principal para otimiza√ß√£o do modelo\n",
    "            \"random_state\": 42,           # Garante reprodutibilidade dos resultados\n",
    "            \"n_jobs\": -1                  # Utiliza todos os n√∫cleos de processamento dispon√≠veis para acelerar a execu√ß√£o\n",
    "        },\n",
    "\n",
    "        # Configura√ß√£o com foco em explora√ß√£o profunda e diversidade m√°xima\n",
    "        {\n",
    "            \"generations\": 30,            # Aumenta o n√∫mero de gera√ß√µes para uma explora√ß√£o mais profunda do espa√ßo de hiperpar√¢metros\n",
    "            \"population_size\": 600,       # Diversidade m√°xima para explorar uma grande variedade de solu√ß√µes\n",
    "            \"verbosity\": 2,               # Detalha o progresso de cada gera√ß√£o\n",
    "            \"scoring\": \"f1_weighted\",     # F1 Score como m√©trica principal para otimiza√ß√£o do modelo\n",
    "            \"random_state\": 42,           # Garantia de resultados consistentes entre execu√ß√µes\n",
    "            \"n_jobs\": -1                  # Maximiza o uso de recursos computacionais para acelerar o processo de busca\n",
    "        },\n",
    "    ],\n",
    "    \"Stacking Classifier\": [\n",
    "         # Combina√ß√£o leve e eficiente\n",
    "        {\n",
    "            \"estimators\": [\n",
    "                (\"rf_pipeline\", Pipeline([\n",
    "                    (\"scaler\", MaxAbsScaler()),\n",
    "                    (\"classifier\", RandomForestClassifier(\n",
    "                        bootstrap=True,\n",
    "                        criterion=\"gini\",\n",
    "                        max_features=1.0,\n",
    "                        min_samples_leaf=8,\n",
    "                        min_samples_split=6,\n",
    "                        n_estimators=100,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                (\"et1_pipeline\", Pipeline([\n",
    "                    (\"classifier\", ExtraTreesClassifier(\n",
    "                        bootstrap=False,\n",
    "                        criterion=\"entropy\",\n",
    "                        max_features=0.55,\n",
    "                        min_samples_leaf=1,\n",
    "                        min_samples_split=4,\n",
    "                        n_estimators=100,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                (\"et2_pipeline\", Pipeline([\n",
    "                    (\"scaler\", MaxAbsScaler()),\n",
    "                    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)),\n",
    "                    (\"classifier\", ExtraTreesClassifier(\n",
    "                        bootstrap=False,\n",
    "                        criterion=\"entropy\",\n",
    "                        max_features=0.4,\n",
    "                        min_samples_leaf=1,\n",
    "                        min_samples_split=2,\n",
    "                        n_estimators=100,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                # Adicionando novos pipelines com RBFSampler e PolynomialFeatures\n",
    "                (\"et3_pipeline\", Pipeline([\n",
    "                    (\"scaler\", MaxAbsScaler()),\n",
    "                    (\"rbf_sampler\", RBFSampler(gamma=0.5)),\n",
    "                    (\"classifier\", ExtraTreesClassifier(\n",
    "                        bootstrap=False,\n",
    "                        criterion=\"gini\",\n",
    "                        max_features=0.25,\n",
    "                        min_samples_leaf=1,\n",
    "                        min_samples_split=6,\n",
    "                        n_estimators=100,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                (\"et4_pipeline\", Pipeline([\n",
    "                    (\"scaler\", MaxAbsScaler()),\n",
    "                    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)),\n",
    "                    (\"classifier\", ExtraTreesClassifier(\n",
    "                        bootstrap=False,\n",
    "                        criterion=\"gini\",\n",
    "                        max_features=0.25,\n",
    "                        min_samples_leaf=1,\n",
    "                        min_samples_split=3,\n",
    "                        n_estimators=100,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ]))\n",
    "            ],\n",
    "            \"final_estimator\": RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "        },\n",
    "\n",
    "           # Configura√ß√£o robusta com pipelines diversificados\n",
    "        {\n",
    "            \"estimators\": [\n",
    "                (\"rf_pipeline\", Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"classifier\", RandomForestClassifier(\n",
    "                        bootstrap=True,\n",
    "                        criterion=\"gini\",\n",
    "                        max_features=0.8,\n",
    "                        min_samples_leaf=4,\n",
    "                        min_samples_split=5,\n",
    "                        n_estimators=200,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                (\"et_pipeline\", Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"classifier\", ExtraTreesClassifier(\n",
    "                        bootstrap=False,\n",
    "                        criterion=\"entropy\",\n",
    "                        max_features=0.65,\n",
    "                        min_samples_leaf=2,\n",
    "                        min_samples_split=3,\n",
    "                        n_estimators=200,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                (\"gb_pipeline\", Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"poly_features\", PolynomialFeatures(degree=3, include_bias=False, interaction_only=False)),\n",
    "                    (\"classifier\", GradientBoostingClassifier(\n",
    "                        n_estimators=150,\n",
    "                        learning_rate=0.05,\n",
    "                        max_depth=5,\n",
    "                        min_samples_split=4,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                (\"hgb_pipeline\", Pipeline([\n",
    "                    (\"scaler\", MinMaxScaler()),\n",
    "                    (\"classifier\", HistGradientBoostingClassifier(\n",
    "                        max_iter=300,\n",
    "                        learning_rate=0.08,\n",
    "                        max_depth=6,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                (\"cb_pipeline\", Pipeline([\n",
    "                    (\"scaler\", MinMaxScaler()),\n",
    "                    (\"classifier\", CatBoostClassifier(\n",
    "                        iterations=200,\n",
    "                        learning_rate=0.1,\n",
    "                        depth=6,\n",
    "                        verbose=False,\n",
    "                        allow_writing_files=False,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                # Novos pipelines robustos com RBFSampler e PolynomialFeatures\n",
    "                (\"et3_pipeline\", Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"rbf_sampler\", RBFSampler(gamma=0.3)),\n",
    "                    (\"classifier\", ExtraTreesClassifier(\n",
    "                        bootstrap=False,\n",
    "                        criterion=\"gini\",\n",
    "                        max_features=0.35,\n",
    "                        min_samples_leaf=2,\n",
    "                        min_samples_split=4,\n",
    "                        n_estimators=150,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                (\"et4_pipeline\", Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"poly_features\", PolynomialFeatures(degree=3, include_bias=False, interaction_only=False)),\n",
    "                    (\"classifier\", ExtraTreesClassifier(\n",
    "                        bootstrap=False,\n",
    "                        criterion=\"gini\",\n",
    "                        max_features=0.3,\n",
    "                        min_samples_leaf=1,\n",
    "                        min_samples_split=2,\n",
    "                        n_estimators=150,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                (\"xgb_pipeline\", Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"classifier\", XGBClassifier(\n",
    "                        n_estimators=300,\n",
    "                        learning_rate=0.05,\n",
    "                        max_depth=6,\n",
    "                        min_child_weight=3,\n",
    "                        subsample=0.9,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ])),\n",
    "                (\"lgbm_pipeline\", Pipeline([\n",
    "                    (\"scaler\", StandardScaler()),\n",
    "                    (\"classifier\", LGBMClassifier(\n",
    "                        n_estimators=200,\n",
    "                        learning_rate=0.1,\n",
    "                        max_depth=6,\n",
    "                        num_leaves=40,\n",
    "                        random_state=42\n",
    "                    ))\n",
    "                ]))\n",
    "            ],\n",
    "            \"final_estimator\": LogisticRegression(\n",
    "                max_iter=3000, solver=\"lbfgs\", random_state=42\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T00:04:03.651073Z",
     "start_time": "2024-11-20T16:51:09.471208Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690d89ce128e45da83c4c4e9912e9300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/5200 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.7471573220505878\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.7471573220505878\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.7471573220505878\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.7471573220505878\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.7493532612570621\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.758445398109056\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.758445398109056\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.7628548477427003\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.7634598795432275\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.7657878496135113\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.7680144133510352\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.7680144133510352\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.7700339397763596\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 16 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 17 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 18 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 19 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 20 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 21 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 22 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 23 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 24 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Generation 25 - Current best internal CV score: 0.7800998350922081\n",
      "\n",
      "Best pipeline: ExtraTreesClassifier(RBFSampler(input_matrix, gamma=0.5), bootstrap=False, criterion=gini, max_features=0.25, min_samples_leaf=1, min_samples_split=6, n_estimators=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'TPOT_Classifier'.\n",
      "Created version '1' of model 'TPOT_Classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c1cb05e3e348c0ae19faad0f86ab6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957ed004c65b48a89adc72dc12291392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/9600 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.7547222017092071\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.7668361570155349\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.7668361570155349\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.7668361570155349\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.7697254713430465\n",
      "\n",
      "Generation 6 - Current best internal CV score: 0.7697254713430465\n",
      "\n",
      "Generation 7 - Current best internal CV score: 0.7697254713430465\n",
      "\n",
      "Generation 8 - Current best internal CV score: 0.7797549717619717\n",
      "\n",
      "Generation 9 - Current best internal CV score: 0.7797549717619717\n",
      "\n",
      "Generation 10 - Current best internal CV score: 0.7797549717619717\n",
      "\n",
      "Generation 11 - Current best internal CV score: 0.7797549717619717\n",
      "\n",
      "Generation 12 - Current best internal CV score: 0.7797549717619717\n",
      "\n",
      "Generation 13 - Current best internal CV score: 0.7797549717619717\n",
      "\n",
      "Generation 14 - Current best internal CV score: 0.7797549717619717\n",
      "\n",
      "Generation 15 - Current best internal CV score: 0.7801613286799786\n",
      "\n",
      "Best pipeline: ExtraTreesClassifier(PolynomialFeatures(input_matrix, degree=2, include_bias=False, interaction_only=False), bootstrap=False, criterion=gini, max_features=0.25, min_samples_leaf=1, min_samples_split=3, n_estimators=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'TPOT_Classifier' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'TPOT_Classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de96723caa342b58769c502a7c8855f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Stacking_Classifier' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'Stacking_Classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3646dae13c724fe9a152180043ab624f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'Stacking_Classifier' already exists. Creating a new version of this model...\n",
      "Created version '6' of model 'Stacking_Classifier'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e6b45297fc4536be90f392de461c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhores Modelos:\n",
      "\n",
      "{'model': 'TPOTClassifier', 'params': {'generations': 15, 'population_size': 600, 'verbosity': 2, 'scoring': 'f1_weighted', 'random_state': 42, 'n_jobs': -1}, 'Accuracy': 0.7879682601880877, 'Precision': 0.7940591937841426, 'Recall': 0.7879682601880877, 'F1 Score': 0.7868891769628368}\n",
      "{'model': 'StackingClassifier', 'params': {'estimators': [('rf_pipeline', Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 RandomForestClassifier(max_features=0.8, min_samples_leaf=4,\n",
      "                                        min_samples_split=5, n_estimators=150,\n",
      "                                        random_state=42))])), ('et_pipeline', Pipeline(steps=[('classifier',\n",
      "                 ExtraTreesClassifier(criterion='entropy', max_features=0.65,\n",
      "                                      min_samples_leaf=2, min_samples_split=3,\n",
      "                                      n_estimators=150, random_state=42))])), ('gb_pipeline', Pipeline(steps=[('poly_features', PolynomialFeatures(include_bias=False)),\n",
      "                ('classifier',\n",
      "                 GradientBoostingClassifier(max_depth=4, min_samples_split=3,\n",
      "                                            random_state=42))])), ('hgb_pipeline', Pipeline(steps=[('scaler', MinMaxScaler()),\n",
      "                ('classifier',\n",
      "                 HistGradientBoostingClassifier(learning_rate=0.08, max_depth=6,\n",
      "                                                max_iter=200,\n",
      "                                                random_state=42))])), ('cb_pipeline', Pipeline(steps=[('classifier',\n",
      "                 <catboost.core.CatBoostClassifier object at 0x000002335D118650>)]))], 'final_estimator': LogisticRegression(max_iter=2000, random_state=42)}, 'Accuracy': 0.7729476880877743, 'Precision': 0.78017497729255, 'Recall': 0.7729476880877743, 'F1 Score': 0.7711326431282545}\n",
      "{'model': 'TPOTClassifier', 'params': {'generations': 25, 'population_size': 200, 'verbosity': 2, 'scoring': 'f1_weighted', 'random_state': 42, 'n_jobs': -1}, 'Accuracy': 0.7723226880877743, 'Precision': 0.7795037007094706, 'Recall': 0.7723226880877743, 'F1 Score': 0.7707780585177605}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparar o ambiente do MLFlow e iniciar o experimento\n",
    "\n",
    "# Configurar o caminho relativo para os artefatos\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "\n",
    "# lista para armazenar os resultados\n",
    "results = []\n",
    "\n",
    "# Iniciar o experimento\n",
    "mlflow.set_experiment(\"exp_projeto_ciclo_3\")\n",
    "\n",
    "# Run para registrar modelos gerados pelo TPOTClassifier\n",
    "with mlflow.start_run(run_name=\"Modelos TPOT Treinados\") as main_run:  # Principal\n",
    "    counter = 0  # Contador para os experimentos\n",
    "    for params in models[\"TPOT AutoML\"]:  # Itera sobre os par√¢metros do TPOT AutoML\n",
    "        counter += 1\n",
    "        with mlflow.start_run(run_name=f\"{counter}. TPOTClassifier\", nested=True):  # Aninhada\n",
    "            # Instanciar e treinar o TPOTClassifier\n",
    "            model = TPOTClassifier(**params)\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            # Obter o pipeline otimizado\n",
    "            best_pipeline = model.fitted_pipeline_\n",
    "\n",
    "            # Avaliar as m√©tricas usando cross_val_score para o pipeline otimizado\n",
    "            accuracy = cross_val_score(best_pipeline, x_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "            precision = cross_val_score(best_pipeline, x_train, y_train, cv=10, scoring='precision_weighted').mean()\n",
    "            recall = cross_val_score(best_pipeline, x_train, y_train, cv=10, scoring='recall_weighted').mean()\n",
    "            f1 = cross_val_score(best_pipeline, x_train, y_train, cv=10, scoring='f1_weighted').mean()\n",
    "\n",
    "            # Registrar os par√¢metros, m√©tricas e o pipeline otimizado\n",
    "            for key, value in params.items():\n",
    "                mlflow.log_param(key, str(value))\n",
    "            mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"Precision\", precision)\n",
    "            mlflow.log_metric(\"Recall\", recall)\n",
    "            mlflow.log_metric(\"F1 Score\", f1)\n",
    "            mlflow.sklearn.log_model(best_pipeline, artifact_path=\"TPOT_Classifier\",\n",
    "                                                    registered_model_name=\"TPOT_Classifier\",\n",
    "                                                    input_example=x_test.head(1))\n",
    "\n",
    "            # Armazenar resultados\n",
    "            results.append({\n",
    "                \"model\": \"TPOTClassifier\",\n",
    "                \"params\": params,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"F1 Score\": f1,\n",
    "            })\n",
    "\n",
    "# Run para registrar modelos gerados pelo StackingClassifier\n",
    "with mlflow.start_run(run_name=\"Modelos Stacking Treinados\") as main_run:  # Principal\n",
    "    counter = 0  # Contador para os experimentos\n",
    "    for params in models[\"Stacking Classifier\"]:  # Itera sobre os par√¢metros do Stacking Classifier\n",
    "        counter += 1\n",
    "        with mlflow.start_run(run_name=f\"{counter}. StackingClassifier\", nested=True):  # Aninhada\n",
    "            # Instanciar e treinar o StackingClassifier\n",
    "            model = StackingClassifier(**params)\n",
    "            model.fit(x_train, y_train)\n",
    "\n",
    "            # Avaliar as m√©tricas usando cross_val_score para o modelo\n",
    "            accuracy = cross_val_score(model, x_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "            precision = cross_val_score(model, x_train, y_train, cv=10, scoring='precision_weighted').mean()\n",
    "            recall = cross_val_score(model, x_train, y_train, cv=10, scoring='recall_weighted').mean()\n",
    "            f1 = cross_val_score(model, x_train, y_train, cv=10, scoring='f1_weighted').mean()\n",
    "\n",
    "            # Registrar os par√¢metros, m√©tricas e o modelo treinado\n",
    "            for key, value in params.items():\n",
    "                mlflow.log_param(key, str(value))\n",
    "            mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"Precision\", precision)\n",
    "            mlflow.log_metric(\"Recall\", recall)\n",
    "            mlflow.log_metric(\"F1 Score\", f1)\n",
    "            mlflow.sklearn.log_model(model, artifact_path=\"Stacking_Classifier\",\n",
    "                                            registered_model_name=\"Stacking_Classifier\",\n",
    "                                            input_example=x_test.head(1))\n",
    "\n",
    "            # Armazenar resultados\n",
    "            results.append({\n",
    "                \"model\": \"StackingClassifier\",\n",
    "                \"params\": params,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"F1 Score\": f1,\n",
    "            })\n",
    "\n",
    "# Selecionar os 3 melhores modelos com base na m√©trica F1 Score\n",
    "best_models = sorted(results, key=lambda x: x[\"F1 Score\"], reverse=True)[:3]\n",
    "\n",
    "print(\"\\nMelhores Modelos:\\n\")\n",
    "for model_info in best_models:\n",
    "    print(model_info)\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üíæ Armazenando Melhor Modelo Com Pipeline"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T00:53:26.089863Z",
     "start_time": "2024-11-21T00:53:21.634492Z"
    }
   },
   "source": [
    "# Configurar o tracking URI relativo para MLFlow\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "\n",
    "# Nome do experimento\n",
    "experiment_name = \"exp_projeto_ciclo_3\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Fun√ß√£o para criar e salvar o pipeline completo no MLFlow\n",
    "def save_pipeline(ml_model, model_name):\n",
    "    \"\"\"\n",
    "    Combina o modelo com um pipeline de pr√©-processamento e salva no MLFlow.\n",
    "    \"\"\"\n",
    "    # Criar o pipeline com etapas de pr√©-processamento\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Imputa√ß√£o de valores nulos\n",
    "        ('scaler', MinMaxScaler()),                   # Normaliza√ß√£o dos dados\n",
    "        ('model', ml_model)                           # Modelo final\n",
    "    ])\n",
    "    pipeline.fit(x_train, y_train)  # Ajusta o pipeline aos dados\n",
    "\n",
    "    # Wrapper para o pipeline, implementando o modelo para ser usado no MLFlow\n",
    "    class PipelineWrapper(PythonModel):\n",
    "        def load_context(self, context):\n",
    "            # Carregar o pipeline treinado\n",
    "            self.pipeline = pipeline\n",
    "\n",
    "        def predict(self, context, model_input):\n",
    "            # Passar os dados pelo pipeline (imputa√ß√£o + escalonamento + modelo)\n",
    "            return self.pipeline.predict(model_input)\n",
    "\n",
    "    # Caminho para salvar o modelo\n",
    "    save_path = \"best_model\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Salvar o modelo no MLFlow\n",
    "    mlflow.pyfunc.save_model(path=save_path, python_model=PipelineWrapper())\n",
    "\n",
    "    print(f\"\\n{model_name} salvo em: {save_path}\")\n",
    "\n",
    "# Obter todos os runs do experimento, ordenando pelo Accuracy em ordem decrescente\n",
    "runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.`F1 Score` DESC\"], max_results=1)\n",
    "\n",
    "if not runs.empty:\n",
    "    # Selecionar o melhor run\n",
    "    best_run = runs.iloc[0]\n",
    "    best_run_id = best_run[\"run_id\"]\n",
    "\n",
    "    # Verificar se h√° hist√≥rico de modelo registrado\n",
    "    if \"tags.mlflow.log-model.history\" in best_run:\n",
    "        log_model_history = json.loads(best_run[\"tags.mlflow.log-model.history\"])\n",
    "        artifact_path = log_model_history[0][\"artifact_path\"]\n",
    "\n",
    "        # Carregar o modelo usando o URI relativo\n",
    "        model_uri = f\"runs:/{best_run_id}/{artifact_path}\"\n",
    "\n",
    "        # Verificar o tipo de modelo\n",
    "        if \"Stacking\" in artifact_path:\n",
    "            print(\"\\nModelo Stacking Classifier carregado, combinando com o pr√©-processamento...\")\n",
    "\n",
    "            # Carregar o modelo Stacking\n",
    "            loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "            # Salvar o pipeline final com Stacking\n",
    "            save_pipeline(loaded_model, \"Pipeline do Stacking Classifier\")\n",
    "\n",
    "        elif \"TPOT\" in artifact_path:\n",
    "            print(\"\\nModelo TPOT AutoML carregado, combinando com o pr√©-processamento...\")\n",
    "\n",
    "            # Carregar o modelo TPOT\n",
    "            loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "            # Salvar o pipeline final com TPOT\n",
    "            save_pipeline(loaded_model, \"Pipeline do TPOT AutoML\")\n",
    "\n",
    "        else:\n",
    "            print(\"\\nModelo carregado n√£o reconhecido. Salvando com pipeline gen√©rico...\")\n",
    "\n",
    "            # Carregar o modelo gen√©rico\n",
    "            loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "            # Salvar o pipeline gen√©rico\n",
    "            save_pipeline(loaded_model, \"Pipeline Gen√©rico\")\n",
    "    else:\n",
    "        print(\"\\nNenhum hist√≥rico de modelo registrado encontrado para este run.\")\n",
    "else:\n",
    "    print(\"\\nNenhum run encontrado para o experimento especificado.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo TPOT AutoML carregado, combinando com o pr√©-processamento...\n",
      "\n",
      "Pipeline do TPOT AutoML salvo em: best_model\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîã Carregando o Modelo Salvo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T00:54:25.297123Z",
     "start_time": "2024-11-21T00:54:25.151935Z"
    }
   },
   "source": [
    "# Carregar o dataset\n",
    "data = pd.read_csv(\"water_potability.csv\")\n",
    "data = data.dropna()\n",
    "\n",
    "# Dividir o dataset em treino e teste\n",
    "x = data.drop(\"Potability\", axis=1)\n",
    "y = data[\"Potability\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=None, stratify=y)\n",
    "\n",
    "# Caminho para o modelo na pasta raiz\n",
    "model_path = \"./best_model\"\n",
    "\n",
    "# Carregar o modelo diretamente a partir da pasta raiz\n",
    "loaded_model = mlflow.pyfunc.load_model(model_path)\n",
    "\n",
    "# Realizar previs√µes com dados brutos (por exemplo, os dados de teste)\n",
    "# Como o pipeline foi salvo, a entrada precisa ser passada diretamente para o modelo,\n",
    "# que j√° deve cuidar da normaliza√ß√£o e outras transforma√ß√µes, se necess√°rio.\n",
    "test_predictions = loaded_model.predict(x_test)\n",
    "\n",
    "# Exibir os dados de entrada (X) e as sa√≠das esperadas (y_test) junto com as previs√µes\n",
    "print(\"Dados de Entrada (X - Features de Teste):\")\n",
    "print(x_test.head())  # Exibe as primeiras 5 linhas de X\n",
    "\n",
    "print(\"\\nSa√≠das Esperadas (y_test - Potabilidade Esperada):\")\n",
    "print(y_test.head())  # Exibe as primeiras 5 linhas de y_test\n",
    "\n",
    "print(\"\\nPrevis√µes do Modelo (y_pred):\")\n",
    "print(test_predictions[:5])  # Exibe as primeiras 5 previs√µes do modelo\n",
    "\n",
    "# Calcular e exibir as m√©tricas de desempenho\n",
    "# Como o modelo j√° tem pr√©-processamento interno, n√£o precisamos passar pelo cross-validation manualmente,\n",
    "# apenas calcular as m√©tricas diretamente com os dados de teste.\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "precision = precision_score(y_test, test_predictions, average='weighted')\n",
    "recall = recall_score(y_test, test_predictions, average='weighted')\n",
    "f1 = f1_score(y_test, test_predictions, average='weighted')\n",
    "\n",
    "# Exibir as m√©tricas\n",
    "print(\"\\nM√©tricas do Modelo Carregado:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de Entrada (X - Features de Teste):\n",
      "             ph    Hardness        Solids  Chloramines     Sulfate  \\\n",
      "2346   7.617033  242.989402  17681.272357     2.855790  298.413238   \n",
      "309   10.041028  113.831112  16266.434445     7.939074  363.866890   \n",
      "2823   6.910765  242.091338   7764.438022     8.045870  440.635509   \n",
      "2895  13.349889  152.776455  18464.900775     6.717973  334.864070   \n",
      "1639   6.758852  218.153477  22540.775167     7.196435  304.680599   \n",
      "\n",
      "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  \n",
      "2346    549.987318       10.065225        76.513170   4.299543  \n",
      "309     557.486061       13.195341        75.233221   3.807563  \n",
      "2823    487.932310       18.376802        45.306539   4.340149  \n",
      "2895    450.846369       17.192564        85.883523   2.531075  \n",
      "1639    382.502700       18.838735        53.815435   3.639972  \n",
      "\n",
      "Sa√≠das Esperadas (y_test - Potabilidade Esperada):\n",
      "2346    1\n",
      "309     1\n",
      "2823    1\n",
      "2895    0\n",
      "1639    1\n",
      "Name: Potability, dtype: int64\n",
      "\n",
      "Previs√µes do Modelo (y_pred):\n",
      "[1 1 1 0 1]\n",
      "\n",
      "M√©tricas do Modelo Carregado:\n",
      "Accuracy: 0.8270\n",
      "Precision: 0.8292\n",
      "Recall: 0.8270\n",
      "F1 Score: 0.8235\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üíª Modelos Registrados no MLflow"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T00:04:40.996036Z",
     "start_time": "2024-11-21T00:04:40.989144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow UI est√° rodando em http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Definir o tracking URI do MLflow\n",
    "mlflow_tracking_uri = 'file:./mlruns'  # Caminho relativo\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "# Iniciar o MLflow UI em um subprocesso separado\n",
    "mlflow_process = subprocess.Popen([\"mlflow\", \"ui\", \"--host\", \"127.0.0.1\", \"--port\", \"5000\"])\n",
    "\n",
    "# Exibir a URL do MLflow UI\n",
    "print(\"MLflow UI est√° rodando em http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T00:32:02.014648Z",
     "start_time": "2024-11-21T00:32:02.010646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow UI foi parado\n"
     ]
    }
   ],
   "source": [
    "# Parar o subprocesso do MLflow UI\n",
    "mlflow_process.terminate()\n",
    "\n",
    "# Confirmar que o MLflow UI foi parado\n",
    "print(\"MLflow UI foi parado\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
